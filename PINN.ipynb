{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oFTwIHv4eVL5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.autograd import grad\n",
        "import torch.nn as nn\n",
        "from numpy import genfromtxt\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "torch.manual_seed(123);\n",
        "torch.cuda.manual_seed(123)\n",
        "np.random.seed(123)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hmc1jc01fnuy"
      },
      "outputs": [],
      "source": [
        "N = 6e6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aseIWjZXwgCA",
        "outputId": "83278256-37e5-4b0c-81f8-a9710f5fe4fa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOAXoZGEwgCB",
        "outputId": "cb04f673-f0e9-48bf-91a9-53a28f077a66"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "device = torch.device('cuda:0')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSAStipewgCB",
        "outputId": "ea9848d7-aaad-40e1-9a1e-cb8a5562d559"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "torch.cuda.device_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YciAfqTUQPD7",
        "outputId": "eb19a4bd-046e-40b3-dbd5-c4fa8dfcbcab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "длина волны =  299\n",
            "\n",
            "starting training...\n",
            "\n",
            "\n",
            "Epoch  0\n",
            "\n",
            "Saving model... Loss is:  tensor(4.1321, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  0\n",
            "dinn.alpha tensor([0.7099], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "dinn.beta tensor([0.5420], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "dinn.gamma tensor([0.1058], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "\n",
            "Epoch  1000\n",
            "\n",
            "Epoch  2000\n",
            "\n",
            "Epoch  3000\n",
            "\n",
            "Epoch  4000\n",
            "\n",
            "Saving model... Loss is:  tensor(0.0541, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  4000\n",
            "dinn.alpha tensor([0.7007], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "dinn.beta tensor([0.5445], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "dinn.gamma tensor([0.0906], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "\n",
            "Epoch  5000\n",
            "\n",
            "Epoch  6000\n",
            "\n",
            "Epoch  7000\n",
            "\n",
            "Epoch  8000\n",
            "\n",
            "Saving model... Loss is:  tensor(0.0401, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  8000\n",
            "dinn.alpha tensor([0.6798], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "dinn.beta tensor([0.5575], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "dinn.gamma tensor([0.0563], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "\n",
            "Epoch  9000\n",
            "\n",
            "Epoch  10000\n",
            "\n",
            "Epoch  11000\n",
            "\n",
            "Epoch  12000\n",
            "\n",
            "Saving model... Loss is:  tensor(0.0263, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  12000\n",
            "dinn.alpha tensor([0.6566], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "dinn.beta tensor([0.5667], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "dinn.gamma tensor([0.0227], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "\n",
            "Epoch  13000\n",
            "\n",
            "Epoch  14000\n",
            "\n",
            "Epoch  15000\n",
            "\n",
            "Epoch  16000\n",
            "\n",
            "Saving model... Loss is:  tensor(0.0160, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  16000\n",
            "dinn.alpha tensor([0.6322], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "dinn.beta tensor([0.5619], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "dinn.gamma tensor([0.0026], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "\n",
            "Epoch  17000\n",
            "\n",
            "Epoch  18000\n",
            "\n",
            "Epoch  19000\n",
            "\n",
            "Epoch  20000\n",
            "\n",
            "Saving model... Loss is:  tensor(0.0088, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  20000\n",
            "dinn.alpha tensor([0.6069], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "dinn.beta tensor([0.5383], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "dinn.gamma tensor([0.0017], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "\n",
            "Epoch  21000\n",
            "\n",
            "Epoch  22000\n",
            "\n",
            "Epoch  23000\n",
            "\n",
            "Epoch  24000\n",
            "\n",
            "Saving model... Loss is:  tensor(0.0045, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  24000\n",
            "dinn.alpha tensor([0.5804], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "dinn.beta tensor([0.5126], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "dinn.gamma tensor([0.0016], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "\n",
            "Epoch  25000\n",
            "\n",
            "Epoch  26000\n",
            "\n",
            "Epoch  27000\n",
            "\n",
            "Epoch  28000\n",
            "\n",
            "Saving model... Loss is:  tensor(0.0023, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  28000\n",
            "dinn.alpha tensor([0.5529], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "dinn.beta tensor([0.4861], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "dinn.gamma tensor([0.0016], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "\n",
            "Epoch  29000\n",
            "\n",
            "Epoch  30000\n",
            "\n",
            "Epoch  31000\n",
            "\n",
            "Epoch  32000\n",
            "\n",
            "Saving model... Loss is:  tensor(0.0014, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  32000\n",
            "dinn.alpha tensor([0.5244], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "dinn.beta tensor([0.4592], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "dinn.gamma tensor([0.0015], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "\n",
            "Epoch  33000\n",
            "\n",
            "Epoch  34000\n",
            "\n",
            "Epoch  35000\n",
            "\n",
            "Epoch  36000\n",
            "\n",
            "Saving model... Loss is:  tensor(0.0010, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  36000\n",
            "dinn.alpha tensor([0.4948], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "dinn.beta tensor([0.4317], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "dinn.gamma tensor([0.0015], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "\n",
            "Epoch  37000\n",
            "\n",
            "Epoch  38000\n",
            "\n",
            "Epoch  39000\n",
            "\n",
            "Epoch  40000\n",
            "\n",
            "Saving model... Loss is:  tensor(0.0009, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  40000\n",
            "dinn.alpha tensor([0.4642], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "dinn.beta tensor([0.4037], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "dinn.gamma tensor([0.0015], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "\n",
            "Epoch  41000\n",
            "\n",
            "Epoch  42000\n",
            "\n",
            "Epoch  43000\n",
            "\n",
            "Epoch  44000\n",
            "\n",
            "Saving model... Loss is:  tensor(0.0007, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  44000\n",
            "dinn.alpha tensor([0.4325], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "dinn.beta tensor([0.3751], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "dinn.gamma tensor([0.0014], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "\n",
            "Epoch  45000\n",
            "\n",
            "Epoch  46000\n",
            "\n",
            "Epoch  47000\n",
            "\n",
            "Epoch  48000\n",
            "\n",
            "Saving model... Loss is:  tensor(0.0006, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  48000\n",
            "dinn.alpha tensor([0.3999], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "dinn.beta tensor([0.3458], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "dinn.gamma tensor([0.0014], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "\n",
            "Epoch  49000\n",
            "\n",
            "Epoch  50000\n",
            "\n",
            "Epoch  51000\n",
            "\n",
            "Epoch  52000\n",
            "\n",
            "Saving model... Loss is:  tensor(0.0005, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  52000\n",
            "dinn.alpha tensor([0.3663], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "dinn.beta tensor([0.3156], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "dinn.gamma tensor([0.0014], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "\n",
            "Epoch  53000\n",
            "\n",
            "Epoch  54000\n",
            "\n",
            "Epoch  55000\n",
            "\n",
            "Epoch  56000\n",
            "\n",
            "Saving model... Loss is:  tensor(0.0004, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  56000\n",
            "dinn.alpha tensor([0.3319], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "dinn.beta tensor([0.2847], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "dinn.gamma tensor([0.0013], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "\n",
            "Epoch  57000\n",
            "\n",
            "Epoch  58000\n",
            "\n",
            "Epoch  59000\n",
            "\n",
            "Epoch  60000\n",
            "\n",
            "Saving model... Loss is:  tensor(0.0003, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  60000\n",
            "dinn.alpha tensor([0.2968], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "dinn.beta tensor([0.2532], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "dinn.gamma tensor([0.0013], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "\n",
            "Epoch  61000\n",
            "\n",
            "Epoch  62000\n",
            "\n",
            "Epoch  63000\n",
            "\n",
            "Epoch  64000\n",
            "\n",
            "Saving model... Loss is:  tensor(0.0003, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  64000\n",
            "dinn.alpha tensor([0.2611], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "dinn.beta tensor([0.2211], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "dinn.gamma tensor([0.0012], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "\n",
            "Epoch  65000\n",
            "\n",
            "Epoch  66000\n",
            "\n",
            "Epoch  67000\n",
            "\n",
            "Epoch  68000\n",
            "\n",
            "Saving model... Loss is:  tensor(0.0002, device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  68000\n",
            "dinn.alpha tensor([0.2250], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "dinn.beta tensor([0.1887], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "dinn.gamma tensor([0.0012], device='cuda:0', grad_fn=<TanhBackward0>)\n",
            "\n",
            "Epoch  69000\n"
          ]
        }
      ],
      "source": [
        "result =[]\n",
        "for i in range(299,300,1):\n",
        "  print('длина волны = ', i)\n",
        "  x = 190\n",
        "  covid_cumulative_cases = pd.read_csv('/content/вторая волная.csv')\n",
        "  covid_cumulative_cases = covid_cumulative_cases[:i]\n",
        "  covid_cumulative_cases = covid_cumulative_cases.rename(columns={'Unnamed: 0': 't'})\n",
        "  S = covid_cumulative_cases['S']\n",
        "  I = covid_cumulative_cases['I']\n",
        "  D = covid_cumulative_cases['D']\n",
        "  R = covid_cumulative_cases['R']\n",
        "  covid_cumulative_cases['t'] = covid_cumulative_cases['t'].astype(float)\n",
        "\n",
        "  cumulative_susceptible = []\n",
        "  cumulative_infected = []\n",
        "  cumulative_dead = []\n",
        "  cumulative_recovered = []\n",
        "  timesteps = []\n",
        "\n",
        "  d1 = covid_cumulative_cases['S']\n",
        "  d2 = covid_cumulative_cases['I']\n",
        "  d3 = covid_cumulative_cases['D']\n",
        "  d4 = covid_cumulative_cases['R']\n",
        "  d5 = covid_cumulative_cases['t']\n",
        "\n",
        "  for item in range(len(d5)):\n",
        "      if item % 1 == 0:\n",
        "          cumulative_susceptible.append(d1[item])\n",
        "          cumulative_infected.append(d2[item])\n",
        "          cumulative_dead.append(d3[item])\n",
        "          cumulative_recovered.append(d4[item])\n",
        "          timesteps.append(d5[item])\n",
        "\n",
        "\n",
        "  class DINN(nn.Module):\n",
        "      def __init__(self, t, S_data, I_data, D_data, R_data):  # [t,S,I,D,R]\n",
        "          super(DINN, self).__init__()\n",
        "          self.N = 6e6  # population size\n",
        "          self.t = torch.tensor(t, requires_grad=True).to(device)\n",
        "          self.t.retain_grad()\n",
        "          self.t_float = self.t.float()\n",
        "          self.t_batch = torch.reshape(self.t_float, (len(self.t), 1)).to(device)\n",
        "          self.S = torch.tensor(S_data).to(device)\n",
        "          self.I = torch.tensor(I_data).to(device)\n",
        "          self.D = torch.tensor(D_data).to(device)\n",
        "          self.R = torch.tensor(R_data).to(device)\n",
        "\n",
        "          self.losses = []\n",
        "          self.save = 3  # which file to save to\n",
        "\n",
        "          self.alpha_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))  # 0.191\n",
        "          self.beta_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))  # 0.05\n",
        "          self.gamma_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))  # 0.0294\n",
        "\n",
        "        # find values for normalization\n",
        "          self.S_max = max(self.S)\n",
        "          self.I_max = max(self.I)\n",
        "          self.D_max = max(self.D)\n",
        "          self.R_max = max(self.R)\n",
        "          self.S_min = min(self.S)\n",
        "          self.I_min = min(self.I)\n",
        "          self.D_min = min(self.D)\n",
        "          self.R_min = min(self.R)\n",
        "\n",
        "        # unnormalize\n",
        "          self.S_hat = (self.S - self.S_min) / (self.S_max - self.S_min)\n",
        "          self.I_hat = (self.I - self.I_min) / (self.I_max - self.I_min)\n",
        "          self.D_hat = (self.D - self.D_min) / (self.D_max - self.D_min)\n",
        "          self.R_hat = (self.R - self.R_min) / (self.R_max - self.R_min)\n",
        "\n",
        "        # matrices (x4 for S,I,D,R) for the gradients\n",
        "          self.m1 = torch.zeros((len(self.t), 4)).to(device)\n",
        "          self.m1[:, 0] = 1\n",
        "          self.m2 = torch.zeros((len(self.t), 4)).to(device)\n",
        "          self.m2[:, 1] = 1\n",
        "          self.m3 = torch.zeros((len(self.t), 4)).to(device)\n",
        "          self.m3[:, 2] = 1\n",
        "          self.m4 = torch.zeros((len(self.t), 4)).to(device)\n",
        "          self.m4[:, 3] = 1\n",
        "\n",
        "        # NN\n",
        "          self.net_sidr = self.Net_sidr()\n",
        "          self.params = list(self.net_sidr.parameters())\n",
        "          self.params.extend(list([self.alpha_tilda, self.beta_tilda, self.gamma_tilda]))\n",
        "\n",
        "    # force parameters to be in a range\n",
        "      @property\n",
        "      def alpha(self):\n",
        "          return torch.tanh(self.alpha_tilda)\n",
        "\n",
        "      @property\n",
        "      def beta(self):\n",
        "          return torch.tanh(self.beta_tilda)\n",
        "\n",
        "      @property\n",
        "      def gamma(self):\n",
        "          return torch.tanh(self.gamma_tilda)\n",
        "\n",
        "    # nets\n",
        "      class Net_sidr(nn.Module):  # input = [t]\n",
        "          def __init__(self):\n",
        "              super(DINN.Net_sidr, self).__init__()\n",
        "              self.fc1 = nn.Linear(1, 200)\n",
        "              self.fc2 = nn.Linear(200, 100)\n",
        "              self.out = nn.Linear(100, 4)  # outputs S, I, D, R\n",
        "\n",
        "          def forward(self, t_batch):\n",
        "              sidr = F.relu(self.fc1(t_batch))\n",
        "              sidr = F.tanh(self.fc2(sidr))\n",
        "              sidr = self.out(sidr)\n",
        "              return sidr\n",
        "\n",
        "      def net_f(self, t_batch):\n",
        "          sidr_hat = self.net_sidr(t_batch)\n",
        "\n",
        "          S_hat, I_hat, D_hat, R_hat = sidr_hat[:, 0], sidr_hat[:, 1], sidr_hat[:, 2], sidr_hat[:, 3]\n",
        "\n",
        "          sidr_hat.backward(self.m1, retain_graph=True)\n",
        "          S_hat_t = self.t.grad.clone()\n",
        "          self.t.grad.zero_()\n",
        "\n",
        "        # I_t\n",
        "          sidr_hat.backward(self.m2, retain_graph=True)\n",
        "          I_hat_t = self.t.grad.clone()\n",
        "          self.t.grad.zero_()\n",
        "\n",
        "        # D_t\n",
        "          sidr_hat.backward(self.m3, retain_graph=True)\n",
        "          D_hat_t = self.t.grad.clone()\n",
        "          self.t.grad.zero_()\n",
        "\n",
        "        # R_t\n",
        "          sidr_hat.backward(self.m4, retain_graph=True)\n",
        "          R_hat_t = self.t.grad.clone()\n",
        "          self.t.grad.zero_()\n",
        "\n",
        "        # unnormalize\n",
        "          S = self.S_min + (self.S_max - self.S_min) * S_hat\n",
        "          I = self.I_min + (self.I_max - self.I_min) * I_hat\n",
        "          D = self.D_min + (self.D_max - self.D_min) * D_hat\n",
        "          R = self.R_min + (self.R_max - self.R_min) * R_hat\n",
        "\n",
        "          f1_hat = S_hat_t - (-(self.alpha / self.N) * S * I) / (self.S_max - self.S_min)\n",
        "          f2_hat = I_hat_t - ((self.alpha / self.N) * S * I - self.beta * I - self.gamma * I) / (self.I_max - self.I_min)\n",
        "          f3_hat = D_hat_t - (self.gamma * I) / (self.D_max - self.D_min)\n",
        "          f4_hat = R_hat_t - (self.beta * I) / (self.R_max - self.R_min)\n",
        "\n",
        "          return f1_hat, f2_hat, f3_hat, f4_hat, S_hat, I_hat, D_hat, R_hat\n",
        "\n",
        "      def train(self, threshold=0.0001, max_epochs=10000):\n",
        "          print('\\nstarting training...\\n')\n",
        "          epoch = 0\n",
        "          while True:\n",
        "            # lists to hold the output (maintain only the final epoch)\n",
        "              S_pred_list = []\n",
        "              I_pred_list = []\n",
        "              D_pred_list = []\n",
        "              R_pred_list = []\n",
        "\n",
        "              f1, f2, f3, f4, S_pred, I_pred, D_pred, R_pred = self.net_f(self.t_batch)\n",
        "              self.optimizer.zero_grad()\n",
        "\n",
        "              S_pred_list.append(self.S_min + (self.S_max - self.S_min) * S_pred)\n",
        "              I_pred_list.append(self.I_min + (self.I_max - self.I_min) * I_pred)\n",
        "              D_pred_list.append(self.D_min + (self.D_max - self.D_min) * D_pred)\n",
        "              R_pred_list.append(self.R_min + (self.R_max - self.R_min) * R_pred)\n",
        "\n",
        "              loss = 0.8 * (torch.mean(torch.square(self.S_hat[:x] - S_pred[:x])) +\n",
        "                         (torch.mean(torch.square(self.I_hat[:x] - I_pred[:x]))) +\n",
        "                         torch.mean(torch.square(self.D_hat[:x] - D_pred[:x])) +\n",
        "                         torch.mean(torch.square(self.R_hat[:x] - R_pred[:x]))) + 0.2 * (\n",
        "                               torch.mean(torch.square(f1[:x])) +\n",
        "                               torch.mean(torch.square(f2[:x])) +\n",
        "                               torch.mean(torch.square(f3[:x])) +\n",
        "                               torch.mean(torch.square(f4[:x]))\n",
        "                           ) + 0.05 * torch.square(I_pred[-1] - 0)\n",
        "\n",
        "              loss.backward()\n",
        "              self.optimizer.step()\n",
        "              self.scheduler.step()\n",
        "\n",
        "              self.losses.append(loss.item())\n",
        "\n",
        "              if loss.item() < threshold:\n",
        "                  break\n",
        "\n",
        "              if epoch % 1000 == 0:\n",
        "                  print('\\nEpoch ', epoch)\n",
        "\n",
        "            # loss + model parameters update\n",
        "              if epoch % 4000 == 0:\n",
        "                # checkpoint save every 100 epochs if the loss is lower\n",
        "                  print('\\nSaving model... Loss is: ', loss)\n",
        "\n",
        "                  print('epoch: ', epoch)\n",
        "                  print('dinn.alpha', dinn.alpha)\n",
        "                  print('dinn.beta', dinn.beta)\n",
        "                  print('dinn.gamma', dinn.gamma)\n",
        "\n",
        "              epoch += 1\n",
        "              if epoch >= max_epochs:\n",
        "                  break\n",
        "\n",
        "          return S_pred_list, I_pred_list, D_pred_list, R_pred_list\n",
        "\n",
        "  dinn = DINN(timesteps, cumulative_susceptible, cumulative_infected, cumulative_dead, cumulative_recovered) .to(device)\n",
        "\n",
        "  learning_rate = 1e-5\n",
        "  optimizer = optim.Adam(dinn.params, lr = learning_rate)\n",
        "  dinn.optimizer = optimizer\n",
        "\n",
        "\n",
        "  scheduler = torch.optim.lr_scheduler.CyclicLR(dinn.optimizer, base_lr=1e-5, max_lr=1e-3, step_size_up=1000, mode=\"exp_range\", gamma=0.85, cycle_momentum=False)\n",
        "\n",
        "  dinn.scheduler = scheduler\n",
        "\n",
        "  device = next(dinn.parameters()).device\n",
        "\n",
        "  S_pred_list, I_pred_list, D_pred_list, R_pred_list = dinn.train(threshold=0.0001, max_epochs=70000)\n",
        "\n",
        "\n",
        "  res = I_pred_list[0].cpu().detach().numpy()\n",
        "\n",
        "  result.append(res)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "OC56_aWAfXbm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6eac2a1-0ee2-4c07-c6ee-3c3561e3cb2e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([ 4029.083 ,  4258.41  ,  4299.017 ,  4356.106 ,  4416.743 ,\n",
              "         4572.1694,  4553.124 ,  4756.735 ,  4837.096 ,  4919.445 ,\n",
              "         4972.4263,  5037.957 ,  5111.1914,  5186.2026,  5258.9805,\n",
              "         5327.303 ,  5390.54  ,  5449.13  ,  5504.207 ,  5557.2725,\n",
              "         5610.0684,  5664.377 ,  5721.76  ,  5783.6396,  5851.2   ,\n",
              "         5925.587 ,  6007.4014,  6097.2373,  6195.275 ,  6301.875 ,\n",
              "         6416.797 ,  6508.842 ,  6534.6484,  6567.894 ,  6607.8706,\n",
              "         6654.115 ,  6706.1006,  6763.201 ,  6824.865 ,  6890.582 ,\n",
              "         6959.7334,  7032.541 ,  7108.1104,  7186.1846,  7266.7017,\n",
              "         7349.326 ,  7433.7646,  7520.43  ,  7608.687 ,  7698.6787,\n",
              "         7790.334 ,  7883.6885,  7978.8325,  8075.8135,  8174.4277,\n",
              "         8274.85  ,  8377.324 ,  8481.553 ,  8587.984 ,  8696.584 ,\n",
              "         8767.008 ,  8818.926 ,  8873.373 ,  8930.035 ,  8988.998 ,\n",
              "         9050.325 ,  9114.295 ,  9180.72  ,  9249.531 ,  9320.818 ,\n",
              "         9394.803 ,  9471.195 ,  9550.15  ,  9631.494 ,  9715.65  ,\n",
              "         9802.295 ,  9891.5625,  9983.888 , 10078.719 , 10176.613 ,\n",
              "        10277.722 , 10381.824 , 10489.307 , 10600.572 , 10715.625 ,\n",
              "        10834.747 , 10958.459 , 11086.504 , 11220.174 , 11359.059 ,\n",
              "        11503.287 , 11654.363 , 11811.88  , 11976.809 , 12149.773 ,\n",
              "        12331.207 , 12521.447 , 12721.443 , 12932.085 , 13154.009 ,\n",
              "        13387.821 , 13634.408 , 13895.183 , 14170.406 , 14461.581 ,\n",
              "        14769.186 , 15095.134 , 15439.395 , 15804.193 , 16189.884 ,\n",
              "        16597.938 , 17030.125 , 17275.508 , 17462.93  , 17678.625 ,\n",
              "        17923.398 , 18199.16  , 18506.914 , 18848.18  , 19223.85  ,\n",
              "        19635.043 , 20082.422 , 20567.156 , 21090.594 , 21651.582 ,\n",
              "        22252.21  , 22891.834 , 23571.39  , 24289.809 , 25048.334 ,\n",
              "        25845.379 , 26680.746 , 27554.844 , 28465.668 , 29412.766 ,\n",
              "        30395.418 , 31412.639 , 32462.521 , 33569.977 , 34977.664 ,\n",
              "        36408.35  , 37860.26  , 39331.656 , 40820.875 , 42325.426 ,\n",
              "        43843.35  , 45372.715 , 46911.465 , 48457.152 , 50008.78  ,\n",
              "        51562.383 , 53117.715 , 54671.73  , 56222.152 , 57768.41  ,\n",
              "        59307.46  , 60836.715 , 62355.555 , 63861.906 , 65353.5   ,\n",
              "        66828.86  , 68285.195 , 69723.266 , 71140.41  , 72533.82  ,\n",
              "        73903.32  , 75247.39  , 76564.81  , 77853.64  , 79113.55  ,\n",
              "        80342.32  , 81540.29  , 82705.56  , 83837.03  , 84933.84  ,\n",
              "        85995.195 , 87021.836 , 88010.57  , 88962.39  , 89876.445 ,\n",
              "        90752.016 , 91588.4   , 92385.555 , 93143.766 , 93860.6   ,\n",
              "        94537.98  , 95174.78  , 95770.89  , 96326.4   , 96841.43  ,\n",
              "        97315.3   , 97748.56  , 98140.56  , 98493.18  , 98804.03  ,\n",
              "        99074.79  , 99306.266 , 99497.6   , 99649.305 , 99762.59  ,\n",
              "        99836.27  , 99871.1   , 99868.02  , 99827.08  , 99748.69  ,\n",
              "        99633.92  , 99482.09  , 99295.07  , 99071.77  , 98813.91  ,\n",
              "        98521.39  , 98195.3   , 97834.86  , 97442.31  , 97017.55  ,\n",
              "        96560.94  , 96072.87  , 95555.02  , 95006.94  , 94429.67  ,\n",
              "        93823.945 , 93189.17  , 92527.62  , 91838.52  , 91123.64  ,\n",
              "        90383.53  , 89618.01  , 88828.01  , 88013.17  , 87175.85  ,\n",
              "        86316.32  , 85434.85  , 84531.28  , 83607.4   , 82663.23  ,\n",
              "        81699.695 , 80717.484 , 79716.35  , 78697.305 , 77660.8   ,\n",
              "        76608.98  , 75540.38  , 74455.805 , 73357.05  , 72243.914 ,\n",
              "        71116.44  , 69977.11  , 68824.06  , 67658.82  , 66482.984 ,\n",
              "        65294.76  , 64097.707 , 62889.742 , 61672.684 , 60446.74  ,\n",
              "        59211.25  , 57969.66  , 56719.668 , 55463.438 , 54199.914 ,\n",
              "        52930.414 , 51657.453 , 50377.508 , 49093.074 , 47804.914 ,\n",
              "        46513.406 , 45218.375 , 43920.26  , 42620.277 , 41318.59  ,\n",
              "        40013.336 , 38709.055 , 37403.77  , 36096.984 , 34791.32  ,\n",
              "        33484.84  , 32180.746 , 30876.68  , 29574.592 , 28274.12  ,\n",
              "        26974.703 , 25677.309 , 24384.791 , 23093.826 , 21807.559 ,\n",
              "        20522.982 , 19242.879 , 17967.95  , 16696.484 , 15428.84  ,\n",
              "        14167.804 , 12911.123 , 11661.25  , 10415.473 ,  9175.984 ,\n",
              "         7942.01  ,  6715.188 ,  5493.8496,  4279.7524], dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(result)"
      ],
      "metadata": {
        "id": "yZg0w1OyfY0h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec52d440-7c57-4400-c8bf-9de07048cb97"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pinn_pred = pd.DataFrame(data={\n",
        "    'S': S_pred_list[0].cpu().detach().numpy(),\n",
        "    'I': I_pred_list[0].cpu().detach().numpy(),\n",
        "    'D': D_pred_list[0].cpu().detach().numpy(),\n",
        "    'R': R_pred_list[0].cpu().detach().numpy()\n",
        "})"
      ],
      "metadata": {
        "id": "aP1OQI8GE82y"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pinn_pred.to_csv('./pinn_pred.csv')"
      ],
      "metadata": {
        "id": "g4AXvWVcFSUo"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TBA3eJC9FYVH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}